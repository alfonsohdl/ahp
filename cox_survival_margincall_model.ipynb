{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****GOAL OF THE MODEL****\n",
    "\n",
    "**The business goal is to estimate the probability that an outstanding exposure will be collected within a given horizon, conditional on its current age and features.**\n",
    "\n",
    "**â€œGiven how long an account has had a margin call open, what is the probability that it will finish paying (i.e., settle its outstanding obligation) within the next X days?â€**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our survival function ð‘†(ð‘¡) means:\n",
    "\n",
    "- *â€œProbability that the margin call is still unpaid after t days.â€*\n",
    "\n",
    "and the cumulative hazard (or 1â€“S(t)) means:\n",
    "\n",
    "- *â€œProbability that the margin call has been settled within t days.â€*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our model, an **event** means the margin call has been fully paid â€” the accountâ€™s outstanding balance has dropped to nearly zero.\n",
    "\n",
    "The model estimates how long it typically takes for clients to clear their obligations and what factors influence faster or slower repayment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55d1420",
   "metadata": {},
   "source": [
    "Each margin call is tracked from its issue date until it is either collected (event_date) or reaches the model cutoff (censor_date).\n",
    "The model learns how the probability of recovery changes with the age of the margin call (age_since_issue) and other risk factors.\n",
    "Observed durations (time between issue and collection/cutoff) provide the foundation for estimating how likely a currently open margin call is to be recovered over the next 30, 150, or 300 days."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f40dee9",
   "metadata": {},
   "source": [
    "The model considers key factors such as account risk profile, issued amount, payment progress, and the age of each margin call. Generally, calls with lower risk ratings, smaller issued amounts, and recent issuance dates have higher probabilities of being collected within the next periods. As calls age or remain largely unpaid, their recovery probability tends to decline. These estimates help identify which pending calls are more or less likely to be recovered over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04457cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from google.cloud import bigquery\n",
    "from lifelines import CoxPHFitter, KaplanMeierFitter\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "# --- Mode selection ---\n",
    "today = pd.Timestamp.today().normalize()\n",
    "\n",
    "# Generate the business calendar for the current month\n",
    "month_days = pd.bdate_range(today.replace(day=1), today + pd.offsets.MonthEnd(0))\n",
    "first_business_day = month_days[0]\n",
    "\n",
    "if today == first_business_day:\n",
    "    mode = \"score\"\n",
    "else:\n",
    "    mode = \"train\"\n",
    "\n",
    "print(f\"âœ… Mode set to: {mode.upper()} | First business day: {first_business_day.date()}\")\n",
    "\n",
    "# --- BigQuery Configuration ---\n",
    "PROJECT_ID = 'example_projectid_dev'\n",
    "DATASET_ID = 'alfonso_dataset'\n",
    "OUTPUT_TABLE = f'{PROJECT_ID}.{DATASET_ID}.model_predictions'\n",
    "OUTPUT_TABLE_AB = f'{PROJECT_ID}.{DATASET_ID}.model_agebucket'\n",
    "\n",
    "# --- File paths ---\n",
    "BASE_DIR = r\"C:\\\\Users\\\\alfonso_hidalgo\\\\Documents\\\\my_rep\\\\my_models\\\\alm\\\\margin_call\"\n",
    "SQL_TRAIN_PATH = \"`train_dataset_sql`\"\n",
    "SQL_SCORE_PATH = \"`score_dataset_sql\"\n",
    "MODEL_PATH = os.path.join(BASE_DIR, 'coxph_model.pkl')\n",
    "\n",
    "# --- Prediction horizons ---\n",
    "prediction_horizons = [30, 150, 300]  # in days\n",
    "\n",
    "# --- Initialize BigQuery client ---\n",
    "client = bigquery.Client(project=PROJECT_ID)\n",
    "\n",
    "print(f\"âœ… Environment ready | MODE: {mode.upper()}\")\n",
    "print(f\"ðŸ“¦ Output table: {OUTPUT_TABLE}\")\n",
    "\n",
    "# ==============================\n",
    "# 2. LOAD SQL AND FETCH DATA\n",
    "# ==============================\n",
    "\n",
    "sql_path = SQL_TRAIN_PATH if mode == \"train\" else SQL_SCORE_PATH\n",
    "print(f\"[INFO] Loading SQL source for mode={mode}: {sql_path}\")\n",
    "\n",
    "# --- Detect if sql_path is a BigQuery table reference or a local file\n",
    "if sql_path.strip().lower().endswith(\".sql\"):\n",
    "    # CASE 1: Local SQL file (development mode)\n",
    "    with open(sql_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        sql_query = file.read()\n",
    "    print(f\"[INFO] Loaded SQL from local file: {sql_path}\")\n",
    "\n",
    "    df = client.query(sql_query).to_dataframe()\n",
    "else:\n",
    "    # CASE 2: BigQuery table reference (production mode)\n",
    "    table_ref = sql_path.strip(\"`\")  # remove backticks if any\n",
    "    print(f\"[INFO] Reading directly from BigQuery table: {table_ref}\")\n",
    "    df = client.query(f\"SELECT * FROM `{table_ref}`\").to_dataframe()\n",
    "    # List of date columns to normalize\n",
    "    date_cols = [\"issued_date\", \"eod_book\", \"censor_date\"]\n",
    "    for col in date_cols:\n",
    "        df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "\n",
    "print(f\"[INFO] Data fetched from BigQuery | Rows: {len(df):,d} | Columns: {df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"dataset_type\")[\"internal_risk_profile_score\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_static = df[df[\"dataset_type\"] == \"static\"].copy()\n",
    "df_landmark = df[df[\"dataset_type\"] == \"landmark\"].copy()\n",
    "\n",
    "print(\"Static shape:\", df_static.shape)\n",
    "print(\"Landmark shape:\", df_landmark.shape)\n",
    "\n",
    "# Confirm event balance\n",
    "print(df_static[\"is_event\"].value_counts(normalize=True))\n",
    "print(df_landmark[\"is_event\"].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.ecdfplot(data=df, x=\"observed_duration\", hue=\"event_type\")\n",
    "plt.title(\"Empirical CDF: Time to Event vs Censored\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 3. DATA PREPARATION - USING NEW SQL OUTPUT (STATIC + LANDMARK)\n",
    "# ============================================================\n",
    "df_prep = df.copy()\n",
    "print(f\"[INFO] Loaded dataset with {len(df_prep)} rows and {len(df_prep.columns)} columns\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3.1. Basic Sanity Filters\n",
    "# ------------------------------------------------------------\n",
    "# Drop rows with missing duration or event info\n",
    "df_prep = df_prep.dropna(subset=[\"observed_duration\", \"is_event\"]).copy()\n",
    "\n",
    "# Remove negative or zero durations\n",
    "df_prep = df_prep[df_prep[\"observed_duration\"] > 0].copy()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3.2. Handle Missing / Noisy Numeric Fields\n",
    "# ------------------------------------------------------------\n",
    "fill_zero_cols = [\"amount_issued\", \"amount_collected\", \"last_outstanding\"]\n",
    "for col in fill_zero_cols:\n",
    "    if col in df_prep.columns:\n",
    "        df_prep[col] = df_prep[col].fillna(0)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3.3. Impute Missing Risk Scores and Normalize\n",
    "# ------------------------------------------------------------\n",
    "if \"internal_risk_profile_score\" in df_prep.columns:\n",
    "    median_risk = df_prep[\"internal_risk_profile_score\"].median()\n",
    "    df_prep[\"internal_risk_profile_score\"] = df_prep[\"internal_risk_profile_score\"].fillna(median_risk)\n",
    "    df_prep[\"risk_weight\"] = (df_prep[\"internal_risk_profile_score\"].clip(0, 100)) / 100\n",
    "else:\n",
    "    df_prep[\"risk_weight\"] = 0.5\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3.4. Create Unified Modeling Variables\n",
    "# ------------------------------------------------------------\n",
    "df_prep[\"event_observed\"] = df_prep[\"is_event\"].astype(int)\n",
    "df_prep[\"time_to_event\"] = df_prep[\"observed_duration\"].astype(float)\n",
    "\n",
    "# Optional: Filter static or landmark subsets\n",
    "df_static = df_prep[df_prep[\"dataset_type\"] == \"static\"].copy()\n",
    "df_landmark = df_prep[df_prep[\"dataset_type\"] == \"landmark\"].copy()\n",
    "\n",
    "print(f\"[INFO] Static subset: {len(df_static):,} rows | Landmark subset: {len(df_landmark):,} rows\")\n",
    "\n",
    "# Quick quality check\n",
    "print(df_prep[[\"dataset_type\", \"is_event\", \"observed_duration\"]].groupby(\"dataset_type\").describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# ðŸ”¹ ADD NEW COVARIATES FOR REPAYMENT MODEL\n",
    "# ==================================================\n",
    "\n",
    "# Progress ratio: how much of the call has been repaid\n",
    "df_prep[\"progress_ratio\"] = (\n",
    "    df_prep[\"amount_collected\"] / df_prep[\"amount_issued\"].replace(0, np.nan)\n",
    ").clip(0, 1).fillna(0)\n",
    "\n",
    "# Remaining ratio: fraction still unpaid\n",
    "df_prep[\"remaining_ratio\"] = (\n",
    "    df_prep[\"last_outstanding\"] / df_prep[\"amount_issued\"].replace(0, np.nan)\n",
    ").clip(0, 1).fillna(0)\n",
    "\n",
    "# Age since issue at the time of snapshot\n",
    "if \"age_since_issue\" in df_prep.columns:\n",
    "    df_prep[\"age_since_issue\"] = df_prep[\"age_since_issue\"].clip(lower=0)\n",
    "else:\n",
    "    df_prep[\"age_since_issue\"] = (\n",
    "        (df_prep[\"eod_book\"] - df_prep[\"issued_date\"]).dt.days\n",
    "    ).clip(lower=0)\n",
    "\n",
    "# Optional: interaction term (risk Ã— amount)\n",
    "df_prep[\"risk_x_amount\"] = df_prep[\"risk_weight\"] * df_prep[\"amount_issued\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(\n",
    "    data=df, x=\"observed_duration\",\n",
    "    hue=\"dataset_type\", bins=50, kde=True, multiple=\"stack\"\n",
    ")\n",
    "plt.title(\"Distribution of Observed Durations: Static vs Landmark\")\n",
    "plt.show()\n",
    "\n",
    "sns.countplot(\n",
    "    data=df, x=\"event_type\", hue=\"dataset_type\"\n",
    ")\n",
    "plt.title(\"Event vs Censored Counts by Dataset Type\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *The static model looks at each accountâ€™s entire history, from when it was issued to when it was either closed, defaulted, or reached the observation cut-off date.\n",
    "It's like looking at someoneâ€™s whole financial life in one go.*\n",
    "\n",
    "- *The landmark model, on the other hand, looks at each account repeatedly through time, creating â€œsnapshotsâ€ (for example, every few months) and predicting what might happen within the next year. This lets us track risk in real time instead of waiting until the end of the loanâ€™s life.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# 4. MODEL TRAINING / LOADING (STATIC + LANDMARK)\n",
    "# ================================================================\n",
    "# --- File paths for saving both models ---\n",
    "static_model_path = os.path.join(BASE_DIR, \"coxph_model_static.pkl\")\n",
    "landmark_model_path = os.path.join(BASE_DIR, \"coxph_model_landmark.pkl\")\n",
    "# --- Define shared CoxPH covariates ---\n",
    "features = [\"risk_weight\", \"amount_band\", \"progress_ratio\", \"remaining_ratio\", \"age_since_issue\", \"risk_x_amount\"]\n",
    "# Helper: Fit and Save a Cox Model\n",
    "def fit_cox_model(df, model_path, label):\n",
    "    print(f\"\\nðŸ§© Training {label.upper()} Cox Proportional Hazards model...\")\n",
    "\n",
    "    # --- Define labels ---\n",
    "    duration_col = \"observed_duration\"\n",
    "    event_col = \"is_event\"\n",
    "\n",
    "    # --- Encode categorical variables ---\n",
    "    df_enc = pd.get_dummies(df[features], columns=[\"amount_band\"], drop_first=False)\n",
    "\n",
    "    # --- Combine with labels ---\n",
    "    df_fit = df_enc.join(df[[duration_col, event_col]])\n",
    "    print(\"[DEBUG] Columns with NaNs:\")\n",
    "    print(df_fit.isna().sum()[df_fit.isna().sum() > 0])\n",
    "\n",
    "    # --- Clean missing or invalid values ---\n",
    "    n_before = len(df_fit)\n",
    "    df_fit = df_fit.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    n_after = len(df_fit)\n",
    "    print(f\"[CLEANUP] Removed {n_before - n_after} rows with NaN or infinite values\")\n",
    "\n",
    "    # --- Fit CoxPH model ---\n",
    "    cph = CoxPHFitter(penalizer=0.01)\n",
    "    cph.fit(df_fit, duration_col=duration_col, event_col=event_col, show_progress=True)\n",
    "\n",
    "    # --- Save model ---\n",
    "    joblib.dump(cph, model_path)\n",
    "    print(f\"âœ… {label.capitalize()} model trained and saved to {model_path}\")\n",
    "    print(f\"Concordance Index ({label}): {cph.concordance_index_:.3f}\\n\")\n",
    "    return cph\n",
    "\n",
    "# Automatically train or load both Cox models\n",
    "if mode == \"train\":\n",
    "    # --- Prepare static dataset ---\n",
    "    df_static_train = df_prep[df_prep[\"dataset_type\"] == \"static\"].copy()\n",
    "    df_static_train[\"time_to_event\"] = df_static_train[\"observed_duration\"].astype(float)\n",
    "    df_static_train[\"event_observed\"] = df_static_train[\"is_event\"].astype(int)\n",
    "    # --- Prepare landmark dataset ---\n",
    "    df_landmark_train = df_prep[df_prep[\"dataset_type\"] == \"landmark\"].copy()\n",
    "    df_landmark_train[\"time_to_event\"] = df_landmark_train[\"observed_duration\"].astype(float)\n",
    "    df_landmark_train[\"event_observed\"] = df_landmark_train[\"is_event\"].astype(int)\n",
    "    # Sanity check\n",
    "    print(f\"\\n[INFO] Static rows: {len(df_static_train):,}, Landmark rows: {len(df_landmark_train):,}\")\n",
    "    print(f\"Static durations valid? {(df_static_train['time_to_event'] > 0).mean():.2%}\")\n",
    "    print(f\"Landmark durations valid? {(df_landmark_train['time_to_event'] > 0).mean():.2%}\")\n",
    "    # --- Fit both models ---\n",
    "    cph_static = fit_cox_model(df_static_train, static_model_path, \"static\")\n",
    "    cph_landmark = fit_cox_model(df_landmark_train, landmark_model_path, \"landmark\")\n",
    "else:\n",
    "    # --- Load pre-trained models for scoring ---\n",
    "    print(\"\\nðŸ“‚ Loading pre-trained CoxPH models...\")\n",
    "    cph_static = joblib.load(static_model_path)\n",
    "    cph_landmark = joblib.load(landmark_model_path)\n",
    "    print(f\"âœ… Models loaded successfully from {BASE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Ensure your main dataframe is loaded as 'df'\n",
    "# (with columns like dataset_type, is_event, observed_duration, risk_weight, etc.)\n",
    "\n",
    "sns.set(style=\"whitegrid\", palette=\"muted\")\n",
    "\n",
    "# 1ï¸âƒ£ â€” Basic overview\n",
    "print(df.groupby(\"dataset_type\")[[\"observed_duration\", \"internal_risk_profile_score\"]].describe())\n",
    "\n",
    "# 2ï¸âƒ£ â€” Distribution comparison for key covariates\n",
    "key_covariates = [\"observed_duration\", \"internal_risk_profile_score\"]\n",
    "\n",
    "for col in key_covariates:\n",
    "    if col not in df.columns:\n",
    "        print(f\"[WARN] Skipping missing column: {col}\")\n",
    "        continue\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.kdeplot(data=df, x=col, hue=\"dataset_type\", fill=True, common_norm=False, alpha=0.4)\n",
    "    plt.title(f\"Distribution of {col}: Static vs Landmark\")\n",
    "    plt.show()\n",
    "\n",
    "# 3ï¸âƒ£ â€” Pairplot to visually check relationships\n",
    "sample_df = df.sample(frac=0.05, random_state=42)  # sample for speed\n",
    "sns.pairplot(sample_df,\n",
    "             vars=[\"observed_duration\", \"internal_risk_profile_score\"],\n",
    "             hue=\"dataset_type\", diag_kind=\"kde\", corner=True)\n",
    "plt.suptitle(\"Pairwise Feature Relationships â€” Static vs Landmark\", y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# 4ï¸âƒ£ â€” Correlation comparison\n",
    "for ds in [\"static\", \"landmark\"]:\n",
    "    corr = df[df[\"dataset_type\"] == ds][\n",
    "        [\"observed_duration\", \"internal_risk_profile_score\"]\n",
    "    ].corr()\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    sns.heatmap(corr, annot=True, cmap=\"coolwarm\", center=0, fmt=\".2f\")\n",
    "    plt.title(f\"Correlation Matrix ({ds.capitalize()})\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Optional: Quick visual diagnostics\n",
    "# ------------------------------------------------\n",
    "if mode == \"train\":\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    cph_static.plot(ax=axes[0])\n",
    "    axes[0].set_title(\"Static Model: log(HR) Â±95% CI\")\n",
    "\n",
    "    cph_landmark.plot(ax=axes[1])\n",
    "    axes[1].set_title(\"Landmark Model: log(HR) Â±95% CI\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot partial effects for comparison\n",
    "    cph_static.plot_partial_effects_on_outcome(\n",
    "        covariates=\"risk_weight\",\n",
    "        values=[0.25, 0.5, 0.75, 1.0],\n",
    "        cmap=\"Blues\",\n",
    "        ax=None,\n",
    "    )\n",
    "    plt.title(\"Static Model â€” Partial Effect of Risk Weight\")\n",
    "    plt.show()\n",
    "\n",
    "    cph_landmark.plot_partial_effects_on_outcome(\n",
    "        covariates=\"risk_weight\",\n",
    "        values=[0.25, 0.5, 0.75, 1.0],\n",
    "        cmap=\"Oranges\",\n",
    "        ax=None,\n",
    "    )\n",
    "    plt.title(\"Landmark Model â€” Partial Effect of Risk Weight\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# 5. MODEL SCORING / PREDICTION (STATIC + LANDMARK)\n",
    "# ===============================================================\n",
    "\n",
    "# --- Define prediction horizons (in days) ---\n",
    "prediction_horizons = [30, 150, 300]  # adjust these as needed\n",
    "\n",
    "# --- Choose subsets for scoring ---\n",
    "df_static_score = df_prep[df_prep[\"dataset_type\"] == \"static\"].copy()\n",
    "df_landmark_score = df_prep[df_prep[\"dataset_type\"] == \"landmark\"].copy()\n",
    "\n",
    "# --- Ensure feature formatting is correct ---\n",
    "def prepare_features(df, cph_model):\n",
    "    df_sc = df.copy()\n",
    "\n",
    "    df_sc[\"time_to_event\"] = df_sc[\"observed_duration\"].astype(float)\n",
    "    df_sc[\"event_observed\"] = df_sc[\"is_event\"].astype(int)\n",
    "    df_sc[\"risk_weight\"] = df_sc[\"risk_weight\"].astype(float)\n",
    "\n",
    "    # One-hot encode amount_band\n",
    "    df_sc = pd.get_dummies(df_sc, columns=[\"amount_band\"], drop_first=False)\n",
    "\n",
    "    # Align columns with model\n",
    "    expected_cols = cph_model.params_.index\n",
    "    for col in expected_cols:\n",
    "        if col not in df_sc.columns:\n",
    "            df_sc[col] = 0\n",
    "    df_sc = df_sc[expected_cols]\n",
    "\n",
    "    return df_sc\n",
    "\n",
    "df_static_enc = prepare_features(df_static_score, cph_static)\n",
    "df_landmark_enc = prepare_features(df_landmark_score, cph_landmark)\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Helper: generate survival predictions for given horizons\n",
    "# ---------------------------------------------------------------\n",
    "def generate_survival_predictions(cph_model, df_enc, horizons, label):\n",
    "    print(f\"\\nðŸ§® Generating survival predictions for {label.upper()} dataset...\")\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Predict only for selected horizons, not full curve\n",
    "    for h in horizons:\n",
    "        surv_probs = cph_model.predict_survival_function(df_enc, times=[h])\n",
    "        df_temp = pd.DataFrame({\n",
    "            \"reference_id\": df_enc.index,\n",
    "            f\"surv_{h}d\": surv_probs.T.iloc[:, 0].values,\n",
    "            \"dataset_type\": label\n",
    "        })\n",
    "\n",
    "        results.append(df_temp)\n",
    "\n",
    "    # Merge all horizon predictions\n",
    "    surv_results = results[0]\n",
    "    for i in range(1, len(results)):\n",
    "        surv_results = surv_results.merge(results[i], on=[\"reference_id\", \"dataset_type\"], how=\"left\")\n",
    "\n",
    "    return surv_results\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Generate predictions for both models\n",
    "# ---------------------------------------------------------------\n",
    "pred_static = generate_survival_predictions(cph_static, df_static_enc, prediction_horizons, \"static\")\n",
    "pred_landmark = generate_survival_predictions(cph_landmark, df_landmark_enc, prediction_horizons, \"landmark\")\n",
    "\n",
    "# Merge them together\n",
    "predictions_all = pd.concat([pred_static, pred_landmark], ignore_index=True)\n",
    "\n",
    "print(\"\\nâœ… Predictions generated successfully!\")\n",
    "print(predictions_all.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(predictions_all, x=\"surv_300d\", hue=\"dataset_type\", bins=50, kde=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ðŸ§  MODEL SCORING PIPELINE â€” PRODUCTION (LANDMARK MODEL)\n",
    "# ============================================================\n",
    "\n",
    "client = bigquery.Client(project=PROJECT_ID)\n",
    "\n",
    "# --- Prediction horizons ---\n",
    "prediction_horizons = [30, 150, 300]  # in days\n",
    "\n",
    "# --- Load pre-trained landmark model ---\n",
    "landmark_model_path = os.path.join(BASE_DIR, \"coxph_model_landmark.pkl\")\n",
    "cph_landmark = joblib.load(landmark_model_path)\n",
    "print(\"âœ… Landmark model loaded successfully.\")\n",
    "\n",
    "# ============================================================\n",
    "# 1ï¸âƒ£ LOAD CURRENT SNAPSHOT FROM BIGQUERY\n",
    "# ============================================================\n",
    "\n",
    "table_ref = SQL_SCORE_PATH.strip(\"`\")\n",
    "print(f\"[INFO] Reading live snapshot from BigQuery table: {table_ref}\")\n",
    "\n",
    "df_score_raw = client.query(f\"SELECT * FROM `{table_ref}`\").to_dataframe()\n",
    "print(f\"[INFO] Loaded {len(df_score_raw):,} live records for scoring.\")\n",
    "\n",
    "# ============================================================\n",
    "# ðŸ§© FEATURE ENGINEERING for score_dataset\n",
    "# ============================================================\n",
    "\n",
    "df_score_raw = client.query(f\"SELECT * FROM `{table_ref}`\").to_dataframe()\n",
    "print(f\"[INFO] Loaded {len(df_score_raw):,} live records for scoring.\")\n",
    "\n",
    "# --- 1. Normalize risk_weight (0â€“1 scale) ---\n",
    "df_score_raw[\"risk_weight\"] = (\n",
    "    df_score_raw[\"internal_risk_profile_score\"].fillna(50).clip(0, 100) / 100\n",
    ")\n",
    "\n",
    "# --- 2. Derive amount_band (categorical) ---\n",
    "df_score_raw[\"amount_band\"] = pd.cut(\n",
    "    df_score_raw[\"gbp_issued\"],\n",
    "    bins=[-float(\"inf\"), 5000, 20000, float(\"inf\")],\n",
    "    labels=[\"Small\", \"Medium\", \"Large\"]\n",
    ")\n",
    "\n",
    "# --- 3. Compute progress_ratio (how much collected) ---\n",
    "df_score_raw[\"progress_ratio\"] = (\n",
    "    df_score_raw[\"gbp_collected\"] / df_score_raw[\"gbp_issued\"]\n",
    ").replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "# --- 4. Compute remaining_ratio (how much still to collect) ---\n",
    "df_score_raw[\"remaining_ratio\"] = 1 - df_score_raw[\"progress_ratio\"]\n",
    "\n",
    "# --- 5. Assign age_since_issue ---\n",
    "df_score_raw[\"age_since_issue\"] = df_score_raw[\"days_from_issued_date\"].fillna(0)\n",
    "\n",
    "# --- 6. Risk Ã— amount interaction ---\n",
    "df_score_raw[\"risk_x_amount\"] = df_score_raw[\"risk_weight\"] * df_score_raw[\"gbp_issued\"]\n",
    "\n",
    "print(\"[INFO] Derived all model features successfully.\")\n",
    "\n",
    "# ============================================================\n",
    "# 2ï¸âƒ£ PREPARE FEATURES\n",
    "# ============================================================\n",
    "\n",
    "def prepare_features(df, cph_model):\n",
    "    df_sc = df.copy()\n",
    "    # --- Keep ID and descriptive columns ---\n",
    "    keep_cols = [\"reference_id\", \"account_name\"]\n",
    "    # Ensure feature types\n",
    "    df_sc[\"risk_weight\"] = df_sc[\"risk_weight\"].astype(float)\n",
    "    df_sc[\"age_since_issue\"] = df_sc[\"age_since_issue\"].astype(float)\n",
    "    \n",
    "    # One-hot encode categorical variables\n",
    "    df_sc = pd.get_dummies(df_sc, columns=[\"amount_band\"], drop_first=False)\n",
    "    \n",
    "    # Align with model expected columns\n",
    "    expected_cols = cph_model.params_.index\n",
    "    for col in expected_cols:\n",
    "        if col not in df_sc.columns:\n",
    "            df_sc[col] = 0\n",
    "    # --- Final order: IDs + model features ---\n",
    "    df_sc = df_sc[keep_cols + list(expected_cols)]\n",
    "    return df_sc\n",
    "\n",
    "df_score_enc = prepare_features(df_score_raw, cph_landmark)\n",
    "print(\"[INFO] Features prepared for production scoring.\")\n",
    "\n",
    "# ============================================================\n",
    "# 3ï¸âƒ£ GENERATE SURVIVAL + COLLECTION PROBABILITIES\n",
    "# ============================================================\n",
    "\n",
    "def generate_survival_predictions(cph_model, df_enc, horizons, label):\n",
    "    results = []\n",
    "    for h in horizons:\n",
    "        surv_probs = cph_model.predict_survival_function(df_enc.drop(columns=[\"reference_id\", \"account_name\"]), times=[h])\n",
    "        df_temp = pd.DataFrame({\n",
    "            \"reference_id\": df_enc[\"reference_id\"].values,\n",
    "            \"account_name\": df_enc[\"account_name\"].values,\n",
    "            f\"surv_{h}d\": surv_probs.T.iloc[:, 0].values,\n",
    "            \"dataset_type\": label\n",
    "        })\n",
    "        results.append(df_temp)\n",
    "\n",
    "    surv_results = results[0]\n",
    "    for i in range(1, len(results)):\n",
    "        surv_results = surv_results.merge(results[i], on=[\"reference_id\", \"account_name\", \"dataset_type\"], how=\"left\")\n",
    "    return surv_results\n",
    "\n",
    "# --- Generate survival predictions ---\n",
    "pred_prod = generate_survival_predictions(cph_landmark, df_score_enc, prediction_horizons, \"production\")\n",
    "\n",
    "# --- Compute collection probabilities ---\n",
    "for h in prediction_horizons:\n",
    "    pred_prod[f\"prob_collected_{h}d\"] = 1 - pred_prod[f\"surv_{h}d\"]\n",
    "\n",
    "# Add timestamp\n",
    "pred_prod[\"scoring_date\"] = datetime.now().date()\n",
    "\n",
    "print(\"âœ… Predictions generated successfully.\")\n",
    "print(pred_prod.head())\n",
    "\n",
    "print(pred_prod[[\"reference_id\", \"account_name\"]].head())\n",
    "print(pred_prod.dtypes)\n",
    "\n",
    "# ============================================================\n",
    "# 4ï¸âƒ£ EXPORT RESULTS BACK TO BIGQUERY\n",
    "# ============================================================\n",
    "\n",
    "job_config = bigquery.LoadJobConfig(write_disposition=\"WRITE_TRUNCATE\")\n",
    "client.load_table_from_dataframe(pred_prod, OUTPUT_TABLE, job_config=job_config).result()\n",
    "print(f\"âœ… Predictions uploaded to BigQuery table: {OUTPUT_TABLE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df_score_raw, x=\"progress_ratio\", bins=50, kde=True)\n",
    "plt.title(\"Progress Ratio Distribution (Live Margin Calls)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Near 0.0 = Outstanding / not collected = Active margin calls still awaiting payment.\n",
    "\n",
    "- Near 1.0 = Fully collected = Margin calls that have already been settled.\n",
    "\n",
    "- Middle (0.2â€“0.8) = Partial collections = Operationally rare, clients tend to pay all or none."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df_score_raw, x=\"age_since_issue\", y=\"progress_ratio\", alpha=0.5)\n",
    "plt.title(\"Progress vs. Age Since Issue (Live Margin Calls)\")\n",
    "plt.xlabel(\"Days since issued\")\n",
    "plt.ylabel(\"Progress ratio (Collected / Issued)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most points are clustered vertically around very low ages (0â€“100 days).\n",
    "\n",
    "Within that cluster, progress ratios are mostly near 0 (unpaid) and 1 (fully paid).\n",
    "\n",
    "A few calls extend beyond 500â€“1000 days, these are long-standing MCs, probably residuals or historical outliers that never fully cleared.\n",
    "\n",
    "âœ… This confirms the pattern saw in the previous histogram: Most live margin calls are either brand new (unpaid) or fully collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ðŸ§® AGE BUCKET ANALYSIS â€“ Weighted Probabilities + Expected Recovery + Total Row\n",
    "# ============================================\n",
    "\n",
    "# --- Define 30-day buckets up to 1 year, then \"Over 1 year\" ---\n",
    "bins = list(range(0, 361, 30)) + [float(\"inf\")]\n",
    "labels = [f\"{i}-{i+29}d\" for i in range(0, 360, 30)] + [\"Over 1 year\"]\n",
    "\n",
    "df_age = df_score_raw.copy()\n",
    "df_age[\"age_bucket\"] = pd.cut(df_age[\"age_since_issue\"], bins=bins, labels=labels, right=False)\n",
    "\n",
    "# --- Assign alphabetical order for sorting ---\n",
    "bucket_order_map = {\n",
    "    \"0-29d\": \"a\", \"30-59d\": \"b\", \"60-89d\": \"c\", \"90-119d\": \"d\",\n",
    "    \"120-149d\": \"e\", \"150-179d\": \"f\", \"180-209d\": \"g\", \"210-239d\": \"h\",\n",
    "    \"240-269d\": \"i\", \"270-299d\": \"j\", \"300-359d\": \"k\", \"Over 1 year\": \"l\"\n",
    "}\n",
    "df_age[\"age_bucket_order\"] = df_age[\"age_bucket\"].map(bucket_order_map)\n",
    "\n",
    "# --- Merge model probabilities ---\n",
    "df_age = df_age.merge(\n",
    "    pred_prod[[\"reference_id\", \"prob_collected_30d\", \"prob_collected_150d\", \"prob_collected_300d\"]],\n",
    "    on=\"reference_id\", how=\"left\"\n",
    ")\n",
    "\n",
    "# --- Compute outstanding exposure globally ---\n",
    "df_age[\"outstanding\"] = df_age[\"gbp_issued\"] - df_age[\"gbp_collected\"]\n",
    "\n",
    "# --- Compute metrics by age bucket ---\n",
    "summary = (\n",
    "    df_age.groupby([\"age_bucket\", \"age_bucket_order\"], observed=True, group_keys=False)\n",
    "    .apply(lambda g: pd.Series({\n",
    "        \"n_accounts\": len(g),\n",
    "\n",
    "        # Average probabilities (unweighted)\n",
    "        \"avg_prob_30d\": g[\"prob_collected_30d\"].mean(),\n",
    "        \"avg_prob_150d\": g[\"prob_collected_150d\"].mean(),\n",
    "        \"avg_prob_300d\": g[\"prob_collected_300d\"].mean(),\n",
    "\n",
    "        # Weighted probabilities (using outstanding as weight)\n",
    "        \"wgt_prob_30d\": (g[\"prob_collected_30d\"] * g[\"outstanding\"]).sum() / g[\"outstanding\"].sum() if g[\"outstanding\"].sum() > 0 else np.nan,\n",
    "        \"wgt_prob_150d\": (g[\"prob_collected_150d\"] * g[\"outstanding\"]).sum() / g[\"outstanding\"].sum() if g[\"outstanding\"].sum() > 0 else np.nan,\n",
    "        \"wgt_prob_300d\": (g[\"prob_collected_300d\"] * g[\"outstanding\"]).sum() / g[\"outstanding\"].sum() if g[\"outstanding\"].sum() > 0 else np.nan,\n",
    "\n",
    "        # Expected recoveries (in Â£)\n",
    "        \"expected_recovery_30d\": (g[\"outstanding\"] * g[\"prob_collected_30d\"]).sum(),\n",
    "        \"expected_recovery_150d\": (g[\"outstanding\"] * g[\"prob_collected_150d\"]).sum(),\n",
    "        \"expected_recovery_300d\": (g[\"outstanding\"] * g[\"prob_collected_300d\"]).sum(),\n",
    "\n",
    "        # Exposure metrics\n",
    "        \"avg_gbp_issued\": g[\"gbp_issued\"].mean(),\n",
    "        \"total_gbp_issued\": g[\"gbp_issued\"].sum(),\n",
    "        \"total_gbp_collected\": g[\"gbp_collected\"].sum(),\n",
    "        \"total_gbp_outstanding\": g[\"outstanding\"].sum(),\n",
    "    }))\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# --- Add traceability metadata ---\n",
    "summary[\"scoring_date\"] = pd.Timestamp.today().normalize()\n",
    "summary[\"scoring_timestamp\"] = datetime.now()\n",
    "summary[\"source_table\"] = \"int_4_cl_mc_score\"\n",
    "summary[\"model_table\"] = \"int_5_cl_mc_predictions\"\n",
    "summary[\"model_version\"] = \"landmark_v1.0\"\n",
    "\n",
    "# --- Ensure BigQuery-compatible types ---\n",
    "for col in [\"age_bucket\", \"age_bucket_order\"]:\n",
    "    summary[col] = summary[col].astype(str)\n",
    "\n",
    "# --- Sort and export ---\n",
    "summary = summary.sort_values(\"age_bucket_order\").reset_index(drop=True)\n",
    "\n",
    "job_config = bigquery.LoadJobConfig(write_disposition=\"WRITE_TRUNCATE\")\n",
    "client.load_table_from_dataframe(summary, OUTPUT_TABLE_AB, job_config=job_config).result()\n",
    "\n",
    "print(f\"\\nâœ… Age bucket summary uploaded successfully to {OUTPUT_TABLE_AB}\")\n",
    "print(f\"Rows uploaded: {len(summary)}, Columns: {len(summary.columns)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the order of buckets explicitly\n",
    "bucket_order = [\n",
    "    \"0-29d\", \"30-59d\", \"60-89d\", \"90-119d\",\n",
    "    \"120-149d\", \"150-179d\", \"180-209d\", \"210-239d\",\n",
    "    \"240-269d\", \"270-299d\", \"300-359d\", \"Over 1 year\"\n",
    "]\n",
    "\n",
    "# Convert to ordered categorical\n",
    "summary[\"age_bucket\"] = pd.Categorical(\n",
    "    summary[\"age_bucket\"],\n",
    "    categories=bucket_order,\n",
    "    ordered=True\n",
    ")\n",
    "\n",
    "# Sort by that order\n",
    "summary = summary.sort_values(\"age_bucket\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(data=summary, x=\"age_bucket\", y=\"wgt_prob_30d\", label=\"Wgt Prob 30d\")\n",
    "sns.lineplot(data=summary, x=\"age_bucket\", y=\"wgt_prob_150d\", label=\"Wgt Prob 150d\")\n",
    "sns.lineplot(data=summary, x=\"age_bucket\", y=\"wgt_prob_300d\", label=\"Wgt Prob 300d\")\n",
    "\n",
    "plt.title(\"Weighted Collection Probability by Age Bucket (Live Margin Calls)\")\n",
    "plt.xlabel(\"Age since issue (buckets)\")\n",
    "plt.ylabel(\"Probability of being collected\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This chart adjusts for the size of each exposure, so it shows the overall portfolioâ€™s expected cash recovery, not just the count of calls.**\n",
    "\n",
    "- Larger margin calls tend to be paid slightly faster, which improves early-bucket performance.\n",
    "\n",
    "- But once a large call remains unpaid for several months, the likelihood of collecting it drops even more sharply.\n",
    "\n",
    "- This means the biggest risks are long-aged, high-value calls, they carry most of the financial exposure but very low recovery potential."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****Summary****\n",
    "\n",
    "- Avg. Coll. Prob. = The longer a margin call stays unpaid, the smaller its chance of ever being recovered, fresh calls matter most.\n",
    "\n",
    "- Wei. Coll. Prob. = Our biggest exposures behave the same way, quick payers clear fast, but old, large calls rarely recover."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
